{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1MQJlZmxkEnu9SpMllXrctLEHjGCthz-A",
      "authorship_tag": "ABX9TyM53smJVtLBlW+ZlOXPdhuX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/konpatel/Pneumonia-detection-xray/blob/master/pneumonia-detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8LZSiJvbo2K",
        "outputId": "0b2d15e4-cfdf-4d95-8afd-84e7e54101a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mplimg\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "from keras import layers\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Conv2D\n",
        "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout,GlobalAveragePooling2D\n",
        "from keras.models import Model,load_model,Sequential\n",
        "# import efficientnet.keras as efn\n",
        "import keras.backend as K\n",
        "\n",
        "from keras.metrics import categorical_accuracy, top_k_categorical_accuracy, categorical_crossentropy\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam,SGD\n",
        "from tensorflow.keras.applications import VGG19\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n",
        "\n"
      ],
      "metadata": {
        "id": "wJtAusMgl-Nq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "traindf=pd.read_csv(\"/content/drive/MyDrive/MachineLearningImages/labels_train.csv\",dtype=str)\n",
        "datagen = ImageDataGenerator(\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.25,\n",
        "        horizontal_flip=True,\n",
        "        validation_split=0.2,\n",
        "        fill_mode='nearest')"
      ],
      "metadata": {
        "id": "XZOhUcIf-GK3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Διάσταση κάθε εικόνας\n",
        "IMAGE_SIZE    = (224, 224)\n",
        "#Αριθμός κατηγοριών που θα γίνει το classification\n",
        "NUM_CLASSES   = 3\n",
        "#Σύνολο εικόνων που θα τροφοδοτούμε το μοντέλο κάθε φορά\n",
        "BATCH_SIZE    = 16  \n",
        "#Αριθμός layer που θα αποκλείσουμε από το CNN\n",
        "FREEZE_LAYERS = 2  \n",
        "#Αριθμός Εποχών\n",
        "NUM_EPOCHS    = 1\n",
        "#Filename του μοντέλου\n",
        "WEIGHTS_FINAL = 'model-final.h5'"
      ],
      "metadata": {
        "id": "3k21ZYRmLvlU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator=datagen.flow_from_dataframe(\n",
        "    dataframe=traindf,\n",
        "    directory=\"/content/drive/MyDrive/MachineLearningImages/train_images/\",\n",
        "    x_col=\"file_name\",\n",
        "    y_col=\"class_id\",\n",
        "    subset=\"training\",\n",
        "    interpolation='nearest',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    seed=42,\n",
        "    shuffle=True,\n",
        "    class_mode=\"categorical\",\n",
        "    target_size=IMAGE_SIZE)\n",
        "\n",
        "valid_generator =datagen.flow_from_dataframe(\n",
        "    dataframe=traindf,\n",
        "    directory=\"/content/drive/MyDrive/MachineLearningImages/train_images\",\n",
        "    x_col=\"file_name\",\n",
        "    y_col=\"class_id\",\n",
        "    subset=\"validation\",\n",
        "    interpolation='nearest',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    seed=42,\n",
        "    shuffle=True,\n",
        "    class_mode=\"categorical\",\n",
        "    target_size=IMAGE_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljn51OxwLy7t",
        "outputId": "34cab499-fa22-4d7e-b043-5faa118ec1b2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3738 validated image filenames belonging to 3 classes.\n",
            "Found 934 validated image filenames belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
        "test_generator=test_datagen.flow_from_directory(    \n",
        "    directory=\"/content/drive/MyDrive/MachineLearningImages/test_images/\",    \n",
        "    batch_size=1,\n",
        "    seed=42,\n",
        "    shuffle=False,\n",
        "    class_mode=None,\n",
        "    target_size=IMAGE_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOimv_P0MGfF",
        "outputId": "6c67df55-3753-4add-a546-6f7d68a04311"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 images belonging to 0 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input\n",
        "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, SeparableConv2D, MaxPool2D, LeakyReLU, Activation\n"
      ],
      "metadata": {
        "id": "qC2T2EO4MUCr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = Input(shape=(224, 224, 3))\n",
        "\n",
        "# First conv block\n",
        "x = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\n",
        "x = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPool2D(pool_size=(2, 2))(x)\n",
        "\n",
        "# Second conv block\n",
        "x = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "x = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPool2D(pool_size=(2, 2))(x)\n",
        "\n",
        "# Third conv block\n",
        "x = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "x = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPool2D(pool_size=(2, 2))(x)\n",
        "\n",
        "# Fourth conv block\n",
        "x = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "x = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPool2D(pool_size=(2, 2))(x)\n",
        "x = Dropout(rate=0.2)(x)\n",
        "\n",
        "# Fifth conv block\n",
        "x = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "x = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPool2D(pool_size=(2, 2))(x)\n",
        "x = Dropout(rate=0.2)(x)\n",
        "\n",
        "# FC layer\n",
        "x = Flatten()(x)\n",
        "x = Dense(units=512, activation='relu')(x)\n",
        "x = Dropout(rate=0.7)(x)\n",
        "x = Dense(units=128, activation='relu')(x)\n",
        "x = Dropout(rate=0.5)(x)\n",
        "x = Dense(units=64, activation='relu')(x)\n",
        "x = Dropout(rate=0.3)(x)\n",
        "\n",
        "# Output layer\n",
        "output = Dense(units=1, activation='sigmoid')(x)\n",
        "\n",
        "# Creating model and compiling\n",
        "model = Model(inputs=inputs, outputs=output)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "checkpoint = ModelCheckpoint(filepath='best_weights.hdf5', save_best_only=True, save_weights_only=True)\n",
        "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=2, mode='max')\n",
        "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=1, mode='min')"
      ],
      "metadata": {
        "id": "62UAqJ8gb3kT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
        "# STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n",
        "# STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
        "\n",
        "max_epochs = 30\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "model.fit(train_generator,\n",
        "                        validation_data = valid_generator,\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=max_epochs,\n",
        "                        verbose=1)\n",
        "\n",
        "# save trained weights\n",
        "model.save(WEIGHTS_FINAL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urYHYvc6TmK7",
        "outputId": "d7e97868-c5be-4e4c-fe2a-8544de162ffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "155/234 [==================>...........] - ETA: 7:30 - loss: 0.6414 - accuracy: 0.6650"
          ]
        }
      ]
    }
  ]
}